{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "googleimage_562project.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dincbariscagri/cng562_project/blob/master/googleimage_562project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVnrdr_ZShJx",
        "colab_type": "text"
      },
      "source": [
        "## Google Drive Authorization for Storage\n",
        "You can reach dincbariscagri@gmail.com to get authorization for google drive storage. I can directly give you the code required.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ixo0-sis0Tit",
        "colab_type": "code",
        "outputId": "8c17c7f1-093a-4a45-b770-2a05f71d5fbf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TECXbokQTXkh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QQTLDzCS7TT",
        "colab_type": "text"
      },
      "source": [
        "## Data Downloading, Adjustments and Storing into Drive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXjt18idTU1F",
        "colab_type": "text"
      },
      "source": [
        "**No need to run these codes.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udM-bNYntdFH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import logging \n",
        "import math\n",
        "import os\n",
        "import subprocess\n",
        "from multiprocessing import Pool\n",
        "from PIL import Image\n",
        "import shutil\n",
        "def create_logger(filename, \n",
        "                  logger_name='logger', \n",
        "                  file_fmt='%(asctime)s %(levelname)-8s: %(message)s',\n",
        "                  console_fmt='%(asctime)s | %(message)s',\n",
        "                  file_level=logging.DEBUG, \n",
        "                  console_level=logging.INFO):\n",
        "    \n",
        "    logger = logging.getLogger(logger_name)\n",
        "    logger.setLevel(logging.DEBUG)\n",
        "    logger.propagate = False\n",
        "\n",
        "    file_fmt = logging.Formatter(file_fmt)\n",
        "    log_file = logging.FileHandler(filename)\n",
        "    log_file.setLevel(file_level)\n",
        "    log_file.setFormatter(file_fmt)\n",
        "    logger.addHandler(log_file)\n",
        "\n",
        "    console_fmt = logging.Formatter(console_fmt)\n",
        "    log_console = logging.StreamHandler()\n",
        "    log_console.setLevel(logging.DEBUG)\n",
        "    log_console.setFormatter(console_fmt)\n",
        "    logger.addHandler(log_console)\n",
        "\n",
        "    return logger\n",
        "\n",
        "\n",
        "def move_images_from_sub_to_root_folder(root_folder, subfolder):\n",
        "    subfolder_content = os.listdir(subfolder)\n",
        "    folders_in_subfolder = [i for i in subfolder_content if os.path.isdir(os.path.join(subfolder, i))]\n",
        "    for folder_in_subfolder in folders_in_subfolder:\n",
        "        subfolder_ = os.path.join(subfolder, folder_in_subfolder)\n",
        "        move_images_from_sub_to_root_folder(root_folder, subfolder_)\n",
        "    images = [i for i in subfolder_content if i not in folders_in_subfolder]\n",
        "    for image in images:\n",
        "        path_to_image = os.path.join(subfolder, image) \n",
        "        os.system(f\"mv {path_to_image} ./{root_folder}/{image}\")\n",
        "        \n",
        "        \n",
        "def remove_all_subfolders_inside_folder(folder):\n",
        "    folder_content = os.listdir(folder)\n",
        "    subfolders = [i for i in folder_content if os.path.isdir(os.path.join(folder, i))]\n",
        "    for subfolder in subfolders:\n",
        "        path_to_subfolder = os.path.join(folder, subfolder)\n",
        "        os.system(f'rm -r {path_to_subfolder}')\n",
        "        \n",
        "        \n",
        "def resize_folder_images(src_dir, dst_dir, size=224):\n",
        "    if not os.path.isdir(dst_dir):\n",
        "        logger.info(\"destination directory does not exist, creating destination directory.\")\n",
        "        os.makedirs(dst_dir)\n",
        "\n",
        "    image_filenames=os.listdir(src_dir)\n",
        "    count = 0\n",
        "    for filename in image_filenames:\n",
        "        dst_filepath = os.path.join(dst_dir, filename)\n",
        "        src_filepath = os.path.join(src_dir, filename)\n",
        "        new_img = read_and_resize_image(src_filepath, size)\n",
        "        if new_img is not None:\n",
        "            new_img = new_img.convert(\"RGB\")\n",
        "            new_img.save(dst_filepath)\n",
        "            count += 1\n",
        "    logger.debug(f'{src_dir} files resized: {count}')\n",
        "    \n",
        "    \n",
        "def read_and_resize_image(filepath, size):\n",
        "    img = read_image(filepath)\n",
        "    if img:\n",
        "        img = resize_image(img, size)\n",
        "    return img\n",
        "\n",
        "\n",
        "def resize_image(img, size):\n",
        "    if type(size) == int:\n",
        "        size = (size, size)\n",
        "    if len(size) > 2:\n",
        "        raise ValueError(\"Size needs to be specified as Width, Height\")\n",
        "    return resize_contain(img, size)\n",
        "\n",
        "\n",
        "def read_image(filepath):\n",
        "    try:\n",
        "        img = Image.open(filepath)\n",
        "        return img\n",
        "    except (OSError, Exception) as e:\n",
        "        logger.debug(\"Can't read file {}\".format(filepath))\n",
        "        return None\n",
        "\n",
        "\n",
        "def resize_contain(image, size, resample=Image.LANCZOS, bg_color=(255, 255, 255, 0)):\n",
        "    img_format = image.format\n",
        "    img = image.copy()\n",
        "    img.thumbnail((size[0], size[1]), resample)\n",
        "    background = Image.new('RGBA', (size[0], size[1]), bg_color)\n",
        "    img_position = (\n",
        "        int(math.ceil((size[0] - img.size[0]) / 2)),\n",
        "        int(math.ceil((size[1] - img.size[1]) / 2))\n",
        "    )\n",
        "    background.paste(img, img_position)\n",
        "    background.format = img_format\n",
        "    return background.convert('RGB')\n",
        "    \n",
        "    \n",
        "def download_resize_clean(index):\n",
        "    try:\n",
        "        if not os.path.exists('train'):\n",
        "            os.system('mkdir train')\n",
        "\n",
        "        file_index = '{0:0>3}'.format(index)\n",
        "        images_file_name = f'images_{file_index}.tar'\n",
        "        images_folder = images_file_name.split('.')[0]\n",
        "        images_md5_file_name = f'md5.images_{file_index}.txt'\n",
        "        images_tar_url = f'https://s3.amazonaws.com/google-landmark/train/{images_file_name}'\n",
        "        images_md5_url = f'https://s3.amazonaws.com/google-landmark/md5sum/train/{images_md5_file_name}'\n",
        "\n",
        "        logger.info(f'Downloading: {images_file_name} and {images_md5_file_name}')\n",
        "        os.system(f'wget {images_tar_url}')\n",
        "        os.system(f'wget {images_md5_url}')\n",
        "\n",
        "        logger.debug(f'Checking file md5 and control md5')\n",
        "        p = subprocess.Popen(\n",
        "            [\"md5sum\", images_file_name], \n",
        "            stdout=subprocess.PIPE, \n",
        "            stderr=subprocess.STDOUT\n",
        "        )\n",
        "        stdout, stderr = p.communicate()\n",
        "        md5_images = stdout.decode(\"utf-8\").split(' ')[0]\n",
        "        md5_control = open(images_md5_file_name).read().split(' ')[0]\n",
        "\n",
        "        if md5_images == md5_control:\n",
        "            logger.debug(f'MD5 are the same: {md5_images}, {md5_control}')\n",
        "            logger.debug(f'Unarchiving images into: {images_folder}')\n",
        "            os.system(f'mkdir {images_folder}')\n",
        "            os.system(f'tar -xf {images_file_name} -C ./{images_folder}/')\n",
        "\n",
        "            logger.debug(f'Moving images into root folder')\n",
        "            move_images_from_sub_to_root_folder(images_folder, images_folder)\n",
        "            remove_all_subfolders_inside_folder(images_folder)\n",
        "\n",
        "            logger.debug(f'Resizing images')\n",
        "            resize_folder_images(\n",
        "                src_dir=images_folder, \n",
        "                dst_dir='train',\n",
        "                size=224\n",
        "            )\n",
        "            os.system(f'rm -r {images_folder}')\n",
        "            os.system(f'rm {images_file_name}')\n",
        "            os.system(f'rm {images_md5_file_name}') \n",
        "        else:\n",
        "            logger.error(f'{images_file_name} was not processed due to md5 missmatch')\n",
        "    except:\n",
        "        logger.error(f'FAILED TO PROCESS {images_file_name}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWWzoVIoteR4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "logger = create_logger('download.log')\n",
        "p = Pool(processes=6)\n",
        "for i in range(1,50):  \n",
        "  p.map(download_resize_clean, range(10*(i-1),10*i))\n",
        "  shutil.make_archive(f'train{i}', 'zip', \"train\")\n",
        "  shutil.copy(f'train{i}.zip',f'gdrive/My Drive/CNG562_Images/')\n",
        "  os.system(f'rm -rf train')\n",
        "p.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qIjVOdwqTIpA",
        "colab_type": "text"
      },
      "source": [
        "## Unzipping from Drive and Other Settings (Class and Data reductions for computation Reduction)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5s8oF_ckQs9D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import zipfile\n",
        "for i in range(1,51):\n",
        "  zip_ref = zipfile.ZipFile(f'/content/gdrive/My Drive/CNG562_Images/train{i}.zip', 'r')\n",
        "  zip_ref.extractall(\"train/\")\n",
        "  zip_ref.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8wJIzTPLeBr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import zipfile\n",
        "\n",
        "zip_ref = zipfile.ZipFile(f'/content/gdrive/My Drive/CNG562_Images/train2test.zip', 'r')\n",
        "zip_ref.extractall(\"train/\")\n",
        "zip_ref.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8VgfajtcsbV",
        "colab_type": "code",
        "outputId": "d56bd6d1-d0ad-4a1f-be18-237ac69689da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "!wget https://s3.amazonaws.com/google-landmark/metadata/train.csv"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-06-14 18:16:40--  https://s3.amazonaws.com/google-landmark/metadata/train.csv\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.138.245\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.138.245|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 525832518 (501M) [text/csv]\n",
            "Saving to: ‘train.csv’\n",
            "\n",
            "train.csv           100%[===================>] 501.47M  47.9MB/s    in 10s     \n",
            "\n",
            "2019-06-14 18:16:51 (49.7 MB/s) - ‘train.csv’ saved [525832518/525832518]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3pQyFGcmceAa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "train = pd.read_csv('train.csv')\n",
        "train.id=train.id.astype(str)+\".jpg\"\n",
        "train.landmark_id=train.landmark_id.astype(str)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNrX88w_dNUD",
        "colab_type": "text"
      },
      "source": [
        "Class Count Reduction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3u-itYubYJdz",
        "colab_type": "code",
        "outputId": "32f2d505-a3b4-4eb3-8918-229d17f7ec24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from collections import Counter\n",
        "NUM_THRESHOLD = 250\n",
        "\n",
        "counts = dict(Counter(train['landmark_id']))\n",
        "landmarks_dict = {x:[] for x in train.landmark_id.unique() if counts[x] >= NUM_THRESHOLD}\n",
        "NUM_CLASSES = len(landmarks_dict)\n",
        "print(\"Total number of valid classes: {}\".format(NUM_CLASSES))\n",
        "\n",
        "i = 0\n",
        "landmark_to_idx = {}\n",
        "idx_to_landmark = []\n",
        "for k in landmarks_dict:\n",
        "    landmark_to_idx[k] = i\n",
        "    idx_to_landmark.append(k)\n",
        "    i += 1\n",
        "\n",
        "all_urls = train['url'].tolist()\n",
        "all_landmarks = train['landmark_id'].tolist()\n",
        "valid_urls_dict = {x[0].split(\"/\")[-1]:landmark_to_idx[x[1]] for x in zip(all_urls, all_landmarks) if x[1] in landmarks_dict}\n",
        "valid_urls_list = [x[0] for x in zip(all_urls, all_landmarks) if x[1] in landmarks_dict]\n",
        "\n",
        "NUM_EXAMPLES = len(valid_urls_list)\n",
        "print(\"Total number of valid examples: {}\".format(NUM_EXAMPLES))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of valid classes: 1067\n",
            "Total number of valid examples: 478577\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHM3pbtfsGip",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train2 = train[pd.DataFrame(train.url.tolist()).isin(valid_urls_list).any(1)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxeDMAScEdSh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir train2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G83xTv9s8cn8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import shutil\n",
        "for index, row in train2.iterrows():\n",
        "  shutil.copyfile(f'train/{row[0]}',f'train2/{row[0]}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Ctc1HkCtZIw",
        "colab_type": "code",
        "outputId": "c2bb0c97-1325-42a1-ea2f-a47bc8d54c5b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import os, os.path\n",
        "len(os.listdir('train/'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "478577"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jaFblRYmISrT",
        "colab_type": "code",
        "outputId": "bf67f124-57f4-4a20-ee61-862511e5832e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "shutil.make_archive(f'train2test', 'zip', \"train2\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/train2test.zip'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tGo1YB3SIaby",
        "colab_type": "code",
        "outputId": "7d13adee-3401-472c-df18-8dd573aa5533",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "shutil.copy(f'train2test.zip',f'gdrive/My Drive/CNG562_Images/')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'gdrive/My Drive/CNG562_Images/train2test.zip'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5YPk0yuTQOT",
        "colab_type": "code",
        "outputId": "d078933c-7c6a-4ad6-d3a5-fdd239e76f2a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale=1./255, shear_range=0.2, zoom_range=0.2, horizontal_flip=True)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_set =train_datagen.flow_from_dataframe(dataframe=train2[:430720],directory=\"train/\",x_col='id',y_col='landmark_id',class_mode='categorical',batch_size=1000,target_size=(32,32))\n",
        "test_set = test_datagen.flow_from_dataframe(dataframe=train2[430720:],directory=\"train/\",x_col='id',y_col='landmark_id',class_mode='categorical',batch_size=1000,target_size=(32,32))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 430720 validated image filenames belonging to 1067 classes.\n",
            "Found 47857 validated image filenames belonging to 1067 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXljdUx0dArE",
        "colab_type": "text"
      },
      "source": [
        "Tensor Core adjusments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQIypZ5ZNvTt",
        "colab_type": "code",
        "outputId": "90eb07ba-c657-4424-dc50-58d79adf5f1f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "import os\n",
        "import pprint\n",
        "import tensorflow as tf\n",
        "\n",
        "if 'COLAB_TPU_ADDR' not in os.environ:\n",
        "  print('ERROR: Not connected to a TPU runtime; please see the first cell in this notebook for instructions!')\n",
        "else:\n",
        "  tpu_address = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "  print ('TPU address is', tpu_address)\n",
        "\n",
        "  with tf.Session(tpu_address) as session:\n",
        "    devices = session.list_devices()\n",
        "    \n",
        "  print('TPU devices:')\n",
        "pprint.pprint(devices)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TPU address is grpc://10.125.175.82:8470\n",
            "TPU devices:\n",
            "[_DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 7882111324951740436),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 6528557291878862604),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 14726621810661986602),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 15900451039166030568),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 12377266099146206416),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 4369043022606031793),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 12756309355972463154),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 8227873495543562732),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 5711221558268729928),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 16779242584498591752),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 9641612792320793321)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvrsN9n_dFx3",
        "colab_type": "text"
      },
      "source": [
        "## Model Build"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S4yFd1oqg8se",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Convolutional Neural Network\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "\n",
        "#Intialization\n",
        "classifier = Sequential()\n",
        "\n",
        "#Convolution\n",
        "classifier.add(Conv2D(32,(3,3),input_shape=(64,64,3),activation='relu'))\n",
        "\n",
        "#Max Pooling / Downsampling\n",
        "classifier.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "#Dropout\n",
        "classifier.add(Dropout(rate=0.25))\n",
        "\n",
        "#2nd Convolution \n",
        "classifier.add(Conv2D(64,(3,3),activation='relu'))\n",
        "\n",
        "#2nd Max Pooling / Downsampling\n",
        "classifier.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "#Flatten\n",
        "classifier.add(Flatten())\n",
        "\n",
        "#Full Connection\n",
        "classifier.add(Dense(256, activation='relu'))\n",
        "classifier.add(Dropout(rate=0.5))\n",
        "classifier.add(Dense(1, activation='sigmoid'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ewho2NifCdMr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "classifier.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTrG_8TVg47M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "opt = tf.train.AdamOptimizer(0.001)\n",
        "classifier.compile(opt,loss='categorical_crossentropy',metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7nJRKgYeN3Ea",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TPU_ADDRESS = \"grpc://\" + os.environ['COLAB_TPU_ADDR']\n",
        "resolver = tf.contrib.cluster_resolver.TPUClusterResolver(TPU_ADDRESS)\n",
        "dist_strategy = tf.contrib.tpu.TPUDistributionStrategy(resolver)\n",
        "tpu_model = tf.contrib.tpu.keras_to_tpu_model(classifier,strategy=dist_strategy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gw8pWsIeg6Vf",
        "colab_type": "code",
        "outputId": "4d4aca76-9429-4b99-d8ee-10b1f01edba2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1020
        }
      },
      "source": [
        "history= tpu_model.fit_generator(train_set, steps_per_epoch=430720, epochs=10, validation_data=test_set, validation_steps=47857)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "INFO:tensorflow:New input shapes; (re-)compiling: mode=train (# of cores 8), [TensorSpec(shape=(64,), dtype=tf.int32, name='core_id_80'), TensorSpec(shape=(64, 32, 32, 3), dtype=tf.float32, name='conv2d_8_input_10'), TensorSpec(shape=(64, 213), dtype=tf.float32, name='dense_9_target_30')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for conv2d_8_input\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 4.658758163452148 secs\n",
            "INFO:tensorflow:Setting weights on TPU model.\n",
            "245/343 [====================>.........] - ETA: 1:38 - loss: 4.8091 - acc: 0.0901INFO:tensorflow:New input shapes; (re-)compiling: mode=train (# of cores 8), [TensorSpec(shape=(34,), dtype=tf.int32, name='core_id_80'), TensorSpec(shape=(34, 32, 32, 3), dtype=tf.float32, name='conv2d_8_input_10'), TensorSpec(shape=(34, 213), dtype=tf.float32, name='dense_9_target_30')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for conv2d_8_input\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 3.591972827911377 secs\n",
            "342/343 [============================>.] - ETA: 0s - loss: 4.7033 - acc: 0.1001INFO:tensorflow:New input shapes; (re-)compiling: mode=eval (# of cores 8), [TensorSpec(shape=(64,), dtype=tf.int32, name='core_id_90'), TensorSpec(shape=(64, 32, 32, 3), dtype=tf.float32, name='conv2d_8_input_10'), TensorSpec(shape=(64, 213), dtype=tf.float32, name='dense_9_target_30')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for conv2d_8_input\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 2.9195921421051025 secs\n",
            "38/39 [============================>.] - ETA: 0s - loss: 4.2270 - acc: 0.1529INFO:tensorflow:New input shapes; (re-)compiling: mode=eval (# of cores 8), [TensorSpec(shape=(3,), dtype=tf.int32, name='core_id_90'), TensorSpec(shape=(3, 32, 32, 3), dtype=tf.float32, name='conv2d_8_input_10'), TensorSpec(shape=(3, 213), dtype=tf.float32, name='dense_9_target_30')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for conv2d_8_input\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 3.0215818881988525 secs\n",
            "39/39 [==============================] - 28s 706ms/step - loss: 4.2283 - acc: 0.1530\n",
            "343/343 [==============================] - 362s 1s/step - loss: 4.7020 - acc: 0.1002 - val_loss: 4.2283 - val_acc: 0.1530\n",
            "Epoch 2/10\n",
            "39/39 [==============================] - 25s 647ms/step - loss: 4.0167 - acc: 0.1819\n",
            "343/343 [==============================] - 281s 819ms/step - loss: 4.2511 - acc: 0.1480 - val_loss: 4.0167 - val_acc: 0.1819\n",
            "Epoch 3/10\n",
            "39/39 [==============================] - 25s 635ms/step - loss: 3.8474 - acc: 0.2069\n",
            "343/343 [==============================] - 271s 790ms/step - loss: 4.0856 - acc: 0.1690 - val_loss: 3.8474 - val_acc: 0.2069\n",
            "Epoch 4/10\n",
            "119/343 [=========>....................] - ETA: 2:32 - loss: 4.0083 - acc: 0.1801"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQKn1zypDAZJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Transfer learning with ResNet50\n",
        "from tensorflow.python.keras.applications import ResNet50"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ESCMJnI9p8j5",
        "colab_type": "code",
        "outputId": "7b86408f-77b8-40cb-b55e-5715f1595301",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "## Two-layer model: 1st ResNet50, 2nd Dense softmax\n",
        "\n",
        "#Initialize \n",
        "model = Sequential()\n",
        "\n",
        "#Add ResNet50 \n",
        "model.add(ResNet50(include_top = False, pooling = 'avg', weights = 'imagenet'))\n",
        "\n",
        "#Add Dense softmax\n",
        "model.add(Dense(1067, activation = 'softmax'))\n",
        "\n",
        "#Fix pre-trained weights. Will only train the last layer\n",
        "model.layers[0].trainable = False\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTPJimZNre0c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QphWtQNGrm7w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Compiled\n",
        "opt = tf.train.AdamOptimizer(0.001)\n",
        "model.compile(opt,loss='categorical_crossentropy',metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFdyMijKr0HT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Early stopping and check-point\n",
        "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "cb_early_stopper = EarlyStopping(monitor = 'val_loss', patience = 3)\n",
        "cb_checkpointer = ModelCheckpoint(filepath = '../working/best.hdf5', monitor = 'val_loss', save_best_only = True, mode = 'auto')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKsrtbEYuMLX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Fit the model\n",
        "\n",
        "\n",
        "fit_history = model.fit_generator(\n",
        "        train_set,\n",
        "        steps_per_epoch=430720,\n",
        "        epochs = 10,\n",
        "        validation_data=test_set,\n",
        "        validation_steps=47857,\n",
        "        callbacks=[cb_checkpointer, cb_early_stopper]\n",
        ")\n",
        "model.load_weights(\"../working/best.hdf5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mpElbpmVskco",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for filename in os.listdir('train/'):\n",
        "  if(train2.id.str.contains(filename).any() == False):\n",
        "    os.remove(f'train/{filename}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dd1cX-XJ7SnF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for filename in os.listdir('train/'):\n",
        "  if(train2.id.str.contains(filename).any()):\n",
        "    shutil.copyfile(f'train/{filename}',f'/content/train2/{filename}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OnvlFJbR8koK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}