{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "googleimage.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dincbariscagri/cng562_project/blob/master/googleimage.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ixo0-sis0Tit",
        "colab_type": "code",
        "outputId": "b296d094-5945-4dd7-9b55-4bfee27d263b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udM-bNYntdFH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import logging \n",
        "import math\n",
        "import os\n",
        "import subprocess\n",
        "from multiprocessing import Pool\n",
        "from PIL import Image\n",
        "import shutil\n",
        "def create_logger(filename, \n",
        "                  logger_name='logger', \n",
        "                  file_fmt='%(asctime)s %(levelname)-8s: %(message)s',\n",
        "                  console_fmt='%(asctime)s | %(message)s',\n",
        "                  file_level=logging.DEBUG, \n",
        "                  console_level=logging.INFO):\n",
        "    \n",
        "    logger = logging.getLogger(logger_name)\n",
        "    logger.setLevel(logging.DEBUG)\n",
        "    logger.propagate = False\n",
        "\n",
        "    file_fmt = logging.Formatter(file_fmt)\n",
        "    log_file = logging.FileHandler(filename)\n",
        "    log_file.setLevel(file_level)\n",
        "    log_file.setFormatter(file_fmt)\n",
        "    logger.addHandler(log_file)\n",
        "\n",
        "    console_fmt = logging.Formatter(console_fmt)\n",
        "    log_console = logging.StreamHandler()\n",
        "    log_console.setLevel(logging.DEBUG)\n",
        "    log_console.setFormatter(console_fmt)\n",
        "    logger.addHandler(log_console)\n",
        "\n",
        "    return logger\n",
        "\n",
        "\n",
        "def move_images_from_sub_to_root_folder(root_folder, subfolder):\n",
        "    subfolder_content = os.listdir(subfolder)\n",
        "    folders_in_subfolder = [i for i in subfolder_content if os.path.isdir(os.path.join(subfolder, i))]\n",
        "    for folder_in_subfolder in folders_in_subfolder:\n",
        "        subfolder_ = os.path.join(subfolder, folder_in_subfolder)\n",
        "        move_images_from_sub_to_root_folder(root_folder, subfolder_)\n",
        "    images = [i for i in subfolder_content if i not in folders_in_subfolder]\n",
        "    for image in images:\n",
        "        path_to_image = os.path.join(subfolder, image) \n",
        "        os.system(f\"mv {path_to_image} ./{root_folder}/{image}\")\n",
        "        \n",
        "        \n",
        "def remove_all_subfolders_inside_folder(folder):\n",
        "    folder_content = os.listdir(folder)\n",
        "    subfolders = [i for i in folder_content if os.path.isdir(os.path.join(folder, i))]\n",
        "    for subfolder in subfolders:\n",
        "        path_to_subfolder = os.path.join(folder, subfolder)\n",
        "        os.system(f'rm -r {path_to_subfolder}')\n",
        "        \n",
        "        \n",
        "def resize_folder_images(src_dir, dst_dir, size=224):\n",
        "    if not os.path.isdir(dst_dir):\n",
        "        logger.info(\"destination directory does not exist, creating destination directory.\")\n",
        "        os.makedirs(dst_dir)\n",
        "\n",
        "    image_filenames=os.listdir(src_dir)\n",
        "    count = 0\n",
        "    for filename in image_filenames:\n",
        "        dst_filepath = os.path.join(dst_dir, filename)\n",
        "        src_filepath = os.path.join(src_dir, filename)\n",
        "        new_img = read_and_resize_image(src_filepath, size)\n",
        "        if new_img is not None:\n",
        "            new_img = new_img.convert(\"RGB\")\n",
        "            new_img.save(dst_filepath)\n",
        "            count += 1\n",
        "    logger.debug(f'{src_dir} files resized: {count}')\n",
        "    \n",
        "    \n",
        "def read_and_resize_image(filepath, size):\n",
        "    img = read_image(filepath)\n",
        "    if img:\n",
        "        img = resize_image(img, size)\n",
        "    return img\n",
        "\n",
        "\n",
        "def resize_image(img, size):\n",
        "    if type(size) == int:\n",
        "        size = (size, size)\n",
        "    if len(size) > 2:\n",
        "        raise ValueError(\"Size needs to be specified as Width, Height\")\n",
        "    return resize_contain(img, size)\n",
        "\n",
        "\n",
        "def read_image(filepath):\n",
        "    try:\n",
        "        img = Image.open(filepath)\n",
        "        return img\n",
        "    except (OSError, Exception) as e:\n",
        "        logger.debug(\"Can't read file {}\".format(filepath))\n",
        "        return None\n",
        "\n",
        "\n",
        "def resize_contain(image, size, resample=Image.LANCZOS, bg_color=(255, 255, 255, 0)):\n",
        "    img_format = image.format\n",
        "    img = image.copy()\n",
        "    img.thumbnail((size[0], size[1]), resample)\n",
        "    background = Image.new('RGBA', (size[0], size[1]), bg_color)\n",
        "    img_position = (\n",
        "        int(math.ceil((size[0] - img.size[0]) / 2)),\n",
        "        int(math.ceil((size[1] - img.size[1]) / 2))\n",
        "    )\n",
        "    background.paste(img, img_position)\n",
        "    background.format = img_format\n",
        "    return background.convert('RGB')\n",
        "    \n",
        "    \n",
        "def download_resize_clean(index):\n",
        "    try:\n",
        "        if not os.path.exists('train'):\n",
        "            os.system('mkdir train')\n",
        "\n",
        "        file_index = '{0:0>3}'.format(index)\n",
        "        images_file_name = f'images_{file_index}.tar'\n",
        "        images_folder = images_file_name.split('.')[0]\n",
        "        images_md5_file_name = f'md5.images_{file_index}.txt'\n",
        "        images_tar_url = f'https://s3.amazonaws.com/google-landmark/train/{images_file_name}'\n",
        "        images_md5_url = f'https://s3.amazonaws.com/google-landmark/md5sum/train/{images_md5_file_name}'\n",
        "\n",
        "        logger.info(f'Downloading: {images_file_name} and {images_md5_file_name}')\n",
        "        os.system(f'wget {images_tar_url}')\n",
        "        os.system(f'wget {images_md5_url}')\n",
        "\n",
        "        logger.debug(f'Checking file md5 and control md5')\n",
        "        p = subprocess.Popen(\n",
        "            [\"md5sum\", images_file_name], \n",
        "            stdout=subprocess.PIPE, \n",
        "            stderr=subprocess.STDOUT\n",
        "        )\n",
        "        stdout, stderr = p.communicate()\n",
        "        md5_images = stdout.decode(\"utf-8\").split(' ')[0]\n",
        "        md5_control = open(images_md5_file_name).read().split(' ')[0]\n",
        "\n",
        "        if md5_images == md5_control:\n",
        "            logger.debug(f'MD5 are the same: {md5_images}, {md5_control}')\n",
        "            logger.debug(f'Unarchiving images into: {images_folder}')\n",
        "            os.system(f'mkdir {images_folder}')\n",
        "            os.system(f'tar -xf {images_file_name} -C ./{images_folder}/')\n",
        "\n",
        "            logger.debug(f'Moving images into root folder')\n",
        "            move_images_from_sub_to_root_folder(images_folder, images_folder)\n",
        "            remove_all_subfolders_inside_folder(images_folder)\n",
        "\n",
        "            logger.debug(f'Resizing images')\n",
        "            resize_folder_images(\n",
        "                src_dir=images_folder, \n",
        "                dst_dir='train',\n",
        "                size=224\n",
        "            )\n",
        "            os.system(f'rm -r {images_folder}')\n",
        "            os.system(f'rm {images_file_name}')\n",
        "            os.system(f'rm {images_md5_file_name}') \n",
        "        else:\n",
        "            logger.error(f'{images_file_name} was not processed due to md5 missmatch')\n",
        "    except:\n",
        "        logger.error(f'FAILED TO PROCESS {images_file_name}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWWzoVIoteR4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "logger = create_logger('download.log')\n",
        "p = Pool(processes=6)\n",
        "for i in range(1,50):  \n",
        "  p.map(download_resize_clean, range(10*(i-1),10*i))\n",
        "  shutil.make_archive(f'train{i}', 'zip', \"train\")\n",
        "  shutil.copy(f'train{i}.zip',f'gdrive/My Drive/CNG562_Images/')\n",
        "  os.system(f'rm -rf train')\n",
        "p.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5s8oF_ckQs9D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import zipfile\n",
        "for i in range(1,51):\n",
        "  zip_ref = zipfile.ZipFile(f'/content/gdrive/My Drive/CNG562_Images/train{i}.zip', 'r')\n",
        "  zip_ref.extractall(\"train/\")\n",
        "  zip_ref.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8VgfajtcsbV",
        "colab_type": "code",
        "outputId": "837283f3-3bd9-448a-bc98-19eb391052c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "!wget https://s3.amazonaws.com/google-landmark/metadata/train.csv\n",
        "!wget https://s3.amazonaws.com/google-landmark/metadata/train_attribution.csv"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-06-14 07:30:40--  https://s3.amazonaws.com/google-landmark/metadata/train.csv\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.115.45\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.115.45|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 525832518 (501M) [text/csv]\n",
            "Saving to: ‘train.csv’\n",
            "\n",
            "train.csv           100%[===================>] 501.47M  60.6MB/s    in 9.0s    \n",
            "\n",
            "2019-06-14 07:30:50 (55.6 MB/s) - ‘train.csv’ saved [525832518/525832518]\n",
            "\n",
            "--2019-06-14 07:30:51--  https://s3.amazonaws.com/google-landmark/metadata/train_attribution.csv\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.129.181\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.129.181|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1011452758 (965M) [text/csv]\n",
            "Saving to: ‘train_attribution.csv’\n",
            "\n",
            "train_attribution.c 100%[===================>] 964.60M  58.9MB/s    in 16s     \n",
            "\n",
            "2019-06-14 07:31:07 (59.7 MB/s) - ‘train_attribution.csv’ saved [1011452758/1011452758]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3pQyFGcmceAa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "train = pd.read_csv('train.csv')\n",
        "train.id=train.id.astype(str)+\".jpg\"\n",
        "train.landmark_id=train.landmark_id.astype(str)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3u-itYubYJdz",
        "colab_type": "code",
        "outputId": "62e5c51e-c86a-4817-95ac-810d1ee0c8c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from collections import Counter\n",
        "NUM_THRESHOLD = 250\n",
        "\n",
        "counts = dict(Counter(train['landmark_id']))\n",
        "landmarks_dict = {x:[] for x in train.landmark_id.unique() if counts[x] >= NUM_THRESHOLD}\n",
        "NUM_CLASSES = len(landmarks_dict)\n",
        "print(\"Total number of valid classes: {}\".format(NUM_CLASSES))\n",
        "\n",
        "i = 0\n",
        "landmark_to_idx = {}\n",
        "idx_to_landmark = []\n",
        "for k in landmarks_dict:\n",
        "    landmark_to_idx[k] = i\n",
        "    idx_to_landmark.append(k)\n",
        "    i += 1\n",
        "\n",
        "all_urls = train['url'].tolist()\n",
        "all_landmarks = train['landmark_id'].tolist()\n",
        "valid_urls_dict = {x[0].split(\"/\")[-1]:landmark_to_idx[x[1]] for x in zip(all_urls, all_landmarks) if x[1] in landmarks_dict}\n",
        "valid_urls_list = [x[0] for x in zip(all_urls, all_landmarks) if x[1] in landmarks_dict]\n",
        "\n",
        "NUM_EXAMPLES = len(valid_urls_list)\n",
        "print(\"Total number of valid examples: {}\".format(NUM_EXAMPLES))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of valid classes: 1067\n",
            "Total number of valid examples: 478577\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHM3pbtfsGip",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train2 = train[pd.DataFrame(train.url.tolist()).isin(valid_urls_list).any(1)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5YPk0yuTQOT",
        "colab_type": "code",
        "outputId": "3e7381c4-bf14-48e3-db82-86c469bcff87",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale=1./255, shear_range=0.2, zoom_range=0.2, horizontal_flip=True)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_set =train_datagen.flow_from_dataframe(dataframe=train2[:430720],directory=\"train/\",x_col='id',y_col='landmark_id',class_mode='categorical',batch_size=1000,target_size=(32,32))\n",
        "test_set = test_datagen.flow_from_dataframe(dataframe=train2[430720:],directory=\"train/\",x_col='id',y_col='landmark_id',class_mode='categorical',batch_size=1000,target_size=(32,32))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 430720 validated image filenames belonging to 1067 classes.\n",
            "Found 47857 validated image filenames belonging to 1067 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJSZpxuUyr_j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Convolutional Neural Network\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense,Dropout\n",
        "from tensorflow.keras import regularizers\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S4yFd1oqg8se",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Intialization\n",
        "classifier = Sequential()\n",
        "\n",
        "#Convolution\n",
        "classifier.add(Conv2D(32,(3,3),input_shape=(32,32,3),activation='relu',kernel_regularizer=regularizers.l1(0.01)))\n",
        "\n",
        "#Max Pooling / Downsampling\n",
        "classifier.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "#Dropout\n",
        "classifier.add(Dropout(rate=0.25))\n",
        "\n",
        "#Convolution\n",
        "classifier.add(Conv2D(64,(3,3), activation='relu'))\n",
        "\n",
        "#Max Pooling / Downsampling\n",
        "classifier.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "#Convolution\n",
        "classifier.add(Conv2D(128,(3,3), activation='relu'))\n",
        "\n",
        "#Max Pooling / Downsampling\n",
        "classifier.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "#Flatten\n",
        "classifier.add(Flatten())\n",
        "\n",
        "#Full Connection\n",
        "classifier.add(Dense(128, activation='softmax'))\n",
        "classifier.add(Dropout(rate=0.5))\n",
        "classifier.add(Dense(1067, activation='softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ewho2NifCdMr",
        "colab_type": "code",
        "outputId": "bc75cd7d-e92f-45b2-ea0f-88d61588e285",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        }
      },
      "source": [
        "classifier.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_6 (Conv2D)            (None, 30, 30, 32)        896       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2 (None, 15, 15, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 15, 15, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 13, 13, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2 (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 4, 4, 128)         73856     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_8 (MaxPooling2 (None, 2, 2, 128)         0         \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 128)               65664     \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 1067)              137643    \n",
            "=================================================================\n",
            "Total params: 296,555\n",
            "Trainable params: 296,555\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTrG_8TVg47M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "opt = tf.train.AdamOptimizer(0.001)\n",
        "classifier.compile(opt,loss='categorical_crossentropy',metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gw8pWsIeg6Vf",
        "colab_type": "code",
        "outputId": "bace556d-07ab-474f-bbcc-19a15d3d77e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "history = classifier.fit_generator(train_set, steps_per_epoch=430720, epochs=10, validation_data=test_set, validation_steps=47857)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "    11/430720 [..............................] - ETA: 365:01:42 - loss: 7.5224 - acc: 0.0036"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQKn1zypDAZJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Transfer learning with ResNet50\n",
        "from tensorflow.python.keras.applications import ResNet50"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ESCMJnI9p8j5",
        "colab_type": "code",
        "outputId": "7b86408f-77b8-40cb-b55e-5715f1595301",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "## Two-layer model: 1st ResNet50, 2nd Dense softmax\n",
        "\n",
        "#Initialize \n",
        "model = Sequential()\n",
        "\n",
        "#Add ResNet50 \n",
        "model.add(ResNet50(include_top = False, pooling = 'avg', weights = 'imagenet'))\n",
        "\n",
        "#Add Dense softmax\n",
        "model.add(Dense(1067, activation = 'softmax'))\n",
        "\n",
        "#Fix pre-trained weights. Will only train the last layer\n",
        "model.layers[0].trainable = False\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTPJimZNre0c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QphWtQNGrm7w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Compiled\n",
        "opt = tf.train.AdamOptimizer(0.001)\n",
        "model.compile(opt,loss='categorical_crossentropy',metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFdyMijKr0HT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Early stopping and check-point\n",
        "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "cb_early_stopper = EarlyStopping(monitor = 'val_loss', patience = 3)\n",
        "cb_checkpointer = ModelCheckpoint(filepath = '../working/best.hdf5', monitor = 'val_loss', save_best_only = True, mode = 'auto')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKsrtbEYuMLX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Fit the model\n",
        "\n",
        "\n",
        "fit_history = model.fit_generator(\n",
        "        train_set,\n",
        "        steps_per_epoch=430720,\n",
        "        epochs = 10,\n",
        "        validation_data=test_set,\n",
        "        validation_steps=47857,\n",
        "        callbacks=[cb_checkpointer, cb_early_stopper]\n",
        ")\n",
        "model.load_weights(\"../working/best.hdf5\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}